{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atNCJ50xFrl3"
   },
   "source": [
    "# Working with Sentinel-2 and Copernicus API\n",
    "\n",
    "Goal of the lecture:\n",
    "1. Visualize multiple bands from the downloaded tile\n",
    "1. Create an RGB image of three bands; one true-color and one false-color composite\n",
    "1. Programmatically download the tile from the open and free [Copernicus Dataspace](https://dataspace.copernicus.eu/) page\n",
    "1. Select a sub-region of the tile by defining a region of interest\n",
    "1. Visualize the spectral signature of different land-use/land-cover classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing multiple bands\n",
    "\n",
    "For simplicity, we will re-use large parts of the code from the last lab.\n",
    "\n",
    "### Code from Lab01 Start ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# some code from that last lab\n",
    "\n",
    "# Provided template to download file; not relevant for course\n",
    "import requests\n",
    "from tqdm.rich import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def download_file_with_progress(url: str, output_file: Path):\n",
    "    \"\"\"\n",
    "    Given a `url` as a String and an `output_file` as a file-path the item will\n",
    "    be downloaded and written to the `output_file`. If the `output_file` already\n",
    "    exists, it will be overwritten.\n",
    "    \"\"\"\n",
    "    s = requests.Session()\n",
    "    chunk_size = 2**20  # mb\n",
    "    with s.get(url, stream=True) as resp:\n",
    "        with open(output_file, \"wb\") as f:\n",
    "            print(f\"Saving to {output_file}\")\n",
    "            for data in tqdm(\n",
    "                resp.iter_content(chunk_size=chunk_size), \n",
    "                total=int(resp.headers.get(\"content-length\", 0)) // chunk_size, \n",
    "                unit=\"MB\",\n",
    "                unit_scale=True,\n",
    "                desc=\"Downloading...\",\n",
    "            ):\n",
    "                f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = Path(\"./data\")\n",
    "data_path.mkdir(exist_ok=True)\n",
    "# For quick prototyping there is on such things as _too many_ asserts!\n",
    "assert data_path.exists, \"Should exist after calling mkdir!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tile_name = Path(\"S2A_MSIL2A_20240308T100841_N0510_R022_T33UUU_20240308T143352\")\n",
    "output_filepath = data_path / tile_name.with_suffix(\".SAFE.zip\")\n",
    "output_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# re-hosted file on TUB-Cloud for fast download during class ~800MB before and after unpacking\n",
    "sentinel_tile_url = \"https://tubcloud.tu-berlin.de/s/2EMnZwypF2pK5XG/download/S2A_MSIL2A_20240308T100841_N0510_R022_T33UUU_20240308T143352.SAFE.zip\"\n",
    "\n",
    "download_file_with_progress(url=sentinel_tile_url, output_file=output_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "zipf = zipfile.ZipFile(output_filepath)\n",
    "zipf.extractall(path=\"data\")\n",
    "unzipped_dir = Path(data_path / tile_name.with_suffix(\".SAFE\"))\n",
    "assert unzipped_dir.exists(), f\"{unzipped_dir} does not exist!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class S2TileReader:\n",
    "    def __init__(self, directory: Path):\n",
    "        \"\"\"\n",
    "        Initialize the reader with a directory containing the SAFE file of a Sentinel-2 product.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        directory : Path\n",
    "            The directory containing the SAFE file of a Sentinel-2 product.\n",
    "        \"\"\"\n",
    "        assert directory.is_dir(), f\"{directory} is not a directory\"\n",
    "        self.image_files = list(directory.glob(f\"**/IMG_DATA/*.jp2\"))\n",
    "        if len(self.image_files) == 0:\n",
    "            self.image_files = list(directory.glob(f\"**/IMG_DATA/R60m/*.jp2\"))\n",
    "            self.image_files.extend(list(directory.glob(f\"**/IMG_DATA/R20m/*.jp2\")))\n",
    "            self.image_files.extend(list(directory.glob(f\"**/IMG_DATA/R10m/*.jp2\")))\n",
    "        self.band2file_mapping = self._bands()\n",
    "        self.bands = sorted(self.band2file_mapping.keys())\n",
    "        print(f\"{len(self.band2file_mapping)} images found in {directory}\")\n",
    "\n",
    "    def _bands(self):\n",
    "        \"\"\"\n",
    "        Extract the band names from the image files and create a mapping from band name to file path.\n",
    "\n",
    "        Example:\n",
    "        {\n",
    "            \"B01\": Path(\"path/to/B01.jp2\"),\n",
    "            \"B02\": Path(\"path/to/B02.jp2\"),\n",
    "            ...\n",
    "        }\n",
    "        or if the product has multiple resolutions:\n",
    "        {\n",
    "            \"B01_60m\": Path(\"path/to/R60m/B01.jp2\"),\n",
    "            \"B02_10m\": Path(\"path/to/R10m/B02.jp2\"),\n",
    "            ...\n",
    "        }\n",
    "        \"\"\"\n",
    "        return {\"_\".join(x.stem.split(\"_\")[2:]):x for x in self.image_files}\n",
    "\n",
    "    def read_band(self, band: str):\n",
    "        \"\"\"\n",
    "        Read the data of a specific band. The data is returned as a numpy array. If the band is a single channel,\n",
    "        the array will have shape (height, width). If the band is a multi-channel band, the array will have shape\n",
    "        (height, width, channels).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        band : str\n",
    "            The name of the band to read. Must be one of the bands in the product.\n",
    "            Use the `bands` attribute to see the available bands.\n",
    "        \"\"\"\n",
    "        assert band in self.bands, f\"Band {band} invalid. Please select one of {self.bands}\"\n",
    "        img_path = self.band2file_mapping[band]\n",
    "        with rasterio.open(self.band2file_mapping[band]) as f:\n",
    "            data = f.read()\n",
    "        if data.shape[0] == 1:\n",
    "            return data.squeeze(0)\n",
    "        elif data.shape[0] == 3:\n",
    "            return np.transpose(data, (1, 2, 0))\n",
    "\n",
    "\n",
    "s2_reader = S2TileReader(unzipped_dir)\n",
    "s2_reader.bands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code from Lab01 End ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# discussion from last time\n",
    "def quant_norm_data(\n",
    "        data: np.ndarray, lower_quant: float = 0.01, upper_quant: float = 0.99\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalize the data by quantiles `lower_quant/upper_quant`.\n",
    "    The quantiles are calculated globally/*across all channels*.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.ndarray\n",
    "        The data to normalize.\n",
    "    lower_quant : float\n",
    "        The lower quantile. Default is 0.01.\n",
    "    upper_quant : float\n",
    "        The upper quantile. Default is 0.99.\n",
    "    \"\"\"\n",
    "    masked_data = np.ma.masked_equal(data, 0)\n",
    "    lq, uq = np.quantile(masked_data.compressed(), (lower_quant, upper_quant))\n",
    "    data = np.clip(data, a_min=lq, a_max=uq)\n",
    "    data = (data - lq) / (uq - lq)\n",
    "    return data\n",
    "\n",
    "\n",
    "def vis(data: np.ndarray, quant_norm: bool = False):\n",
    "    \"\"\"\n",
    "    Visualize an array by calling `imshow` with `cmap=\"gray\"` for 1 channel inputs and no cmap for 3 channel inputs.\n",
    "    Expected shape is either (H, W) or (H, W, 3) for 1 and 3 channel inputs respectively. Assumes RGB order for 3\n",
    "    channel inputs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.ndarray\n",
    "        The data to visualize. Expected shape is either (H, W) or (H, W, 3) for 1 and 3 channel inputs respectively.\n",
    "        Assumes RGB order for 3 channel inputs.\n",
    "    quant_norm : bool\n",
    "        Whether to quantile normalize the data. Default is False.\n",
    "    \"\"\"\n",
    "    if quant_norm:\n",
    "        data = quant_norm_data(data)\n",
    "    if data.ndim == 2:\n",
    "        plt.imshow(data, cmap=\"gray\")\n",
    "    elif data.ndim == 3:\n",
    "        plt.imshow(data)\n",
    "    else:\n",
    "        raise ValueError(f\"Expected data to have 2 or 3 dimensions, but got {data.ndim} dimensions.\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rgb_arr = np.stack(\n",
    "    [s2_reader.read_band(b) for b in (\"B04_60m\", \"B03_60m\", \"B02_60m\")],\n",
    "    axis=-1,\n",
    ")\n",
    "rgb_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For 3-dimensional input,\n",
    "# matplotlib's plot function does NOT imply a min-max-normalization!\n",
    "vis(rgb_arr, quant_norm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our _quantile normalizaiton_ strategy, provides very good visual results without too much hand-tuning required.\n",
    "\n",
    "But beware of the difference between our underlying *data* and the\n",
    "visual interpretation! They are not the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vis(rgb_arr, quant_norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course exercise\n",
    "\n",
    "Create a [False Color Composite](https://en.wikipedia.org/wiki/False_color) by visualizing the bands `B8A`, `B04`, `B03`.\n",
    "Try not use the `vis` function. If you have additional time, see what happens if you apply a _manual min-max_ normalization instead!\n",
    "\n",
    "Note: We will talk about the meaning and application of false color composites in the future. For now, the goal is to familiarize yourself with `numpy`, matrix-operations, and visualization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copernicus API\n",
    "\n",
    "Take a look at [Copernicus Dataspace](https://dataspace.copernicus.eu/) and make sure you have created an account and have the correct log-in data available. If you haven't already:\n",
    "1. Sign up for the service \n",
    "2. Confirm the registration e-mail\n",
    "\n",
    "In the following section, we will access the service over its [API](https://en.wikipedia.org/wiki/API)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the different tile data we don't need the access token, but to later download a tile, we need it. \n",
    "The token is valid for only 1 hour, so we cannot reuse it between sessions and have to create it new each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "def get_access_token(username: str, password: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the access token for the Copernicus Data Store. This token is required to access the data for download.\n",
    "    The token is not required for querying the data. It is valid for 3600 seconds (1 hour).\n",
    "\n",
    "    ----------\n",
    "    username : str\n",
    "        The username for the Copernicus Data Store.\n",
    "    password : str\n",
    "        The password for the Copernicus Data Store.\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        \"client_id\": \"cdse-public\",\n",
    "        \"username\": username,\n",
    "        \"password\": password,\n",
    "        \"grant_type\": \"password\",\n",
    "    }\n",
    "    try:\n",
    "        r = requests.post(\n",
    "            \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\",\n",
    "            data=data,\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "    except Exception as e:\n",
    "        raise Exception(\n",
    "            f\"Access token creation failed. Response from the server was: {r.json()}\"\n",
    "        )\n",
    "    return r.json()[\"access_token\"]\n",
    "\n",
    "# you may enter the credentials directly in your notebook\n",
    "user_name = \"\"\n",
    "password = \"\"\n",
    "\n",
    "# the following code is for us to not have to share our secrets ;)\n",
    "user_p = Path(\"user.txt\")\n",
    "pwd_p = Path(\"secret.txt\")\n",
    "if user_p.exists():\n",
    "    user_name = user_p.read_text().strip()\n",
    "if pwd_p.exists():\n",
    "    password = pwd_p.read_text().strip()\n",
    "\n",
    "assert user_name != \"\", \"Please provide your user-name!\"\n",
    "assert password != \"\", \"Please provide your password!\"\n",
    "\n",
    "# return the access token\n",
    "access_token = get_access_token(user_name, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Optional, Iterable, Tuple\n",
    "\n",
    "def dataspace_dataframe_from_attributes(\n",
    "    collection: str = \"SENTINEL-2\",\n",
    "    aoi: Optional[str] = None,\n",
    "    start_date: Optional[str] = None,\n",
    "    end_date: Optional[str] = None,\n",
    "    attributes: Optional[Iterable[Tuple[str, str, float]]] = None,\n",
    "    max_returned_items: int = 20\n",
    "):\n",
    "    \"\"\"\n",
    "    Get a dataframe of items from the Copernicus DataSpace API based on the given attributes.\n",
    "    The request is build based on the OData standard as documented at\n",
    "    https://documentation.dataspace.copernicus.eu/APIs/OData.html\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    collection : str\n",
    "        The collection to search for. Default is \"SENTINEL-2\".\n",
    "    aoi : str, optional\n",
    "        The area of interest in WKT format. Default is None.\n",
    "    start_date : str, optional\n",
    "        The start date in the format \"YYYY-MM-DD\". Default is None.\n",
    "    end_date : str, optional\n",
    "        The end date in the format \"YYYY-MM-DD\". Default is None.\n",
    "    attributes : Iterable[Tuple[str, str, float]], optional\n",
    "        The attributes to filter by. Default is None which means no filtering and is equivalent to an empty list.\n",
    "        Each tuple should be in the format (key, comparison, value).\n",
    "        The comparison should be one of \"lt\", \"le\", \"eq\", \"ge\", \"gt\".\n",
    "        Currently only attributes of type double and that are comparable are supported.\n",
    "    max_returned_items : int, optional\n",
    "        The maximum number of items to return. Default is 20. Must be in [0, 1000].\n",
    "    \"\"\"\n",
    "    if attributes is None:\n",
    "        attributes = []\n",
    "    request_str = \"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=\"\n",
    "    request_str += f\"Collection/Name eq '{collection}'\"\n",
    "    if aoi is not None:\n",
    "        request_str += f\" and OData.CSC.Intersects(area=geography'SRID=4326;{aoi}')\"\n",
    "    if start_date is not None:\n",
    "        request_str += f\" and ContentDate/Start gt {start_date}T00:00:00.000Z\"\n",
    "    if end_date is not None:\n",
    "        request_str += f\" and ContentDate/Start lt {end_date}T00:00:00.000Z\"\n",
    "    for k, comp, v in attributes:\n",
    "        assert comp in [\"lt\", \"le\", \"eq\", \"ge\", \"gt\"]\n",
    "        request_str += f\" and Attributes/OData.CSC.DoubleAttribute/any(att:att/Name eq '{k}' and att/OData.CSC.DoubleAttribute/Value {comp} {v:.2f})\"\n",
    "    # get all attributes\n",
    "    request_str += \"&$expand=Attributes\"\n",
    "    # get top n items\n",
    "    assert 0 <= max_returned_items <= 1000, f\"Copernicus API only allows returned items in [0, 1000], but {max_returned_items} is outside this range.\"\n",
    "    request_str += f\"&$top={max_returned_items}\"\n",
    "    json_result = requests.get(request_str).json()\n",
    "    json_vals = json_result['value']\n",
    "    return pd.DataFrame.from_dict(json_result['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's see what this function does\n",
    "?dataspace_dataframe_from_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataspace_dataframe_from_attributes(attributes=[('cloudCover', 'le', 40)], max_returned_items=1)['Attributes'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataspace_dataframe_from_attributes()['Attributes'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We can see that key-word arguments and attributes can be provided. The attributes are filtered server-side. Right now, only attributes of type double are supported. To see the attributes of a certain product, you have to look into the `'Attributes'`-column of the respective data frame. Not all products have all attributes assigned.\n",
    "\n",
    "To access a range for an attribute, two values have to be inserted, once with upper and once with lower bound\n",
    "\n",
    "### Course exercise\n",
    "Given the available information, construct a query that has the following properties:\n",
    "- Start date is `2023-10-30` and end date is `2023-10-31`\n",
    "- Cloud coverage is between 1.5 and 2.2\n",
    "- Satellite Platform: `SENTINEL-2`\n",
    "\n",
    "You should get back 351 query results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "products = ...\n",
    "\n",
    "# If you get an Error, check if you haven't enabled your account by clicking on the activation e-mail yet or your token has to be renewed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert len(products) == 351, f\"Expected 351 results, got {len(products)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easier to work with the data, we will transform it from a [pandas](https://pandas.pydata.org/) [DataFrame](https://pandas.pydata.org/docs/user_guide/index.html), into a DataFrame that is aware of geographical meta-data and shapes, [geopandas](https://geopandas.org/en/stable/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "def copernicus_df_to_gdf(copernicus_df: pd.DataFrame, strftime: Optional[str] = None) -> gpd.GeoDataFrame:\n",
    "    if strftime is not None and strftime.lower() in ['iso', 'iso 8601', 'default']:\n",
    "        strftime = '%Y-%m-%dT%H:%M:%S'\n",
    "    df = copernicus_df.copy()\n",
    "    # split footprint into geometry (wkt text) and crs\n",
    "    # assign crs\n",
    "    crss = df['Footprint'].apply(lambda x: \"EPSG:\"+x.split(';')[0].split(\"'\")[1].split('=')[1])\n",
    "    assert len(crss.unique()) == 1, \"Multiple CRS values in data frame, geopandas can't handle that.\"\n",
    "    # assign geometry\n",
    "    df['geometry'] = gpd.GeoSeries.from_wkt(df['Footprint'].apply(lambda x: x.split(';')[1].split(\"'\")[0]), crs=crss.iloc[0])\n",
    "    gdf = gpd.GeoDataFrame(df)\n",
    "    attribute_names = set(sl['Name'] for l in gdf['Attributes'] for sl in l)\n",
    "    ignored_date_conversions = []\n",
    "    successful_date_conversions = []\n",
    "    for attr in attribute_names:\n",
    "        gdf[attr] = gdf['Attributes'].apply(lambda x: [i for i in x if i['Name'] == attr][0]['Value'] if len([i for i in x if i['Name'] == attr]) > 0 else np.NaN)\n",
    "    for attr in gdf.columns:\n",
    "        if 'Date' in attr:\n",
    "            # convert to DateTime\n",
    "            try:\n",
    "                gdf[attr] = pd.to_datetime(gdf[attr], yearfirst=True, format='mixed')\n",
    "                # convert to numeric or string to make it json-serializable\n",
    "                if strftime is None:\n",
    "                    gdf[attr] = gdf[attr].values.astype(np.int64) // 10 ** 9\n",
    "                else:\n",
    "                    gdf[attr] = gdf[attr].dt.strftime(strftime)\n",
    "            except Exception as e:\n",
    "                ignored_date_conversions += [attr]\n",
    "                continue\n",
    "            successful_date_conversions += [attr]\n",
    "    print(f\"Ignored date conversions: {ignored_date_conversions}\")\n",
    "    print(f\"Successful date conversions: {successful_date_conversions}\")\n",
    "    return gdf\n",
    "    \n",
    "gdf = copernicus_df_to_gdf(products, strftime='ISO')\n",
    "gdf.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to only display the first 2\n",
    "gdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select a subset of the columns for clarity; these are way too many\n",
    "gdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_gdf = gdf[[\"Name\", \"Id\", \"geometry\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geopandas\n",
    "\n",
    "When you have a `GeoDataFrame` with a `geometry` column set (as provided by the `SentinelAPI`) you can quickly visualize your geographical data. This is one of the main benefits why we use `GeoDataFrame` over a simple Pandas `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_gdf.head(5).explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_gdf.head(1).explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Geopandas in short\n",
    "\n",
    "A [GeoDataFrame](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.html#geopandas.GeoDataFrame) contains one special column and one _special_ property.\n",
    "The _geometry_ column and the [CRS](https://en.wikipedia.org/wiki/Spatial_reference_system) column.\n",
    "\n",
    "##### Geometries/Shapes\n",
    "\n",
    "The geometry column contains _shapes_ in reference to [Coordinate Reference System](https://en.wikipedia.org/wiki/Spatial_reference_system).\n",
    "Let's look at the geometry columns first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# by default it does some nice plotting\n",
    "geometry = sub_gdf.iloc[0][\"geometry\"]\n",
    "geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is the data structure represented?\n",
    "\n",
    "Simply by defining the type of shape (here `POLYGON` or `MultiPolygon`) and the X-Y coordinates of each point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(geometry.wkt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to [JSON](https://en.wikipedia.org/wiki/JSON) these shapes/geometries have their own file format(s) that are used to serialize/deserialize the data, called [WKT](https://en.wikipedia.org/wiki/Well-known_text_representation_of_geometry) (Well-known text representation of geometry).\n",
    "The exact structure of the data format is not too important (nor their alternatives), the main thing to remember is that geometries may be defined by simple textual description, such as `POINT (30 10)` or `LINESTRING (30 10, 10 30, 40 40)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "point = geometry.representative_point()\n",
    "point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "point.wkt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Coordinate Reference System\n",
    "\n",
    "Now let's take a quick look at the [Coordinate Reference System](https://en.wikipedia.org/wiki/Spatial_reference_system).\n",
    "Confusingly, a popular format that is used to describe the coordinate reference system, is also called [WKT or WKT-CRS](https://en.wikipedia.org/wiki/Well-known_text_representation_of_coordinate_reference_systems) (Well-known text representation of coordinate reference systems):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(sub_gdf.crs.to_wkt(pretty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most coordinate reference systems can also be encoded as an [EPSG code](https://en.wikipedia.org/wiki/EPSG_Geodetic_Parameter_Dataset) which is a numerical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_gdf.crs.to_epsg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, it is not important to understand the data format in detail!\n",
    "The main thing to remember is that in order to specify _any_ point in space, we not only need the coordinates but also information to what our axis/scale relate to.\n",
    "For our use-cases the main number to remember is the EPSG code *4326* which is used to describe (Long, Lat) points on earth. \n",
    "\n",
    "#### Using GeoPandas with Copernicus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1960,
     "status": "ok",
     "timestamp": 1618818334836,
     "user": {
      "displayName": "Mat Rb",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gik24jvA_yZMp2z50oFh_gbhSlIRnt_v5WBVKY6REE=s64",
      "userId": "17685210069082378191"
     },
     "user_tz": -120
    },
    "id": "-wm7ZCBvHUCZ",
    "outputId": "8d26a7ec-8d09-40ae-b57d-b2b1741de1e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from shapely.geometry import Point\n",
    "import rasterio\n",
    "\n",
    "from typing import Sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select a point of interest (POI).\n",
    "For this lab we will use the TU Berlin building.\n",
    "\n",
    "The following code requires the Latitude and Longitude coordinates of the POI.\n",
    "For now, we can simply rely on a service like [LatLong.net](latlong.net) to look up the coordinates:\n",
    "\n",
    "- [TU Berlin coordinates via LatLong.net](https://www.latlong.net/place/technical-university-of-berlin-germany-248.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "latitude = 52.51388\n",
    "longitude = 13.32593"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a GeoSeries is just one data column of a GeoDataFrame\n",
    "# or you can think of it as a series/list of geographical objects and a CRS\n",
    "?geopandas.GeoSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "series = geopandas.GeoSeries([Point(longitude, latitude)], crs=\"EPSG:4326\")\n",
    "series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GeoPandas allows us to very quickly visually inspect our geographical data\n",
    "series.explore(marker_type=\"marker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To query the copernicus database, we need to encode our geometry data into a common format that the service can understand.\n",
    "Looking at the documentation of the query function, we can see that the API expect the data to be encoded as a WKT string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "series.to_wkt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "poi = series.to_wkt()[0]\n",
    "poi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_date = date(year=2024, month=3, day=7)\n",
    "end_date = date(year=2024, month=3, day=9)\n",
    "satellite = \"SENTINEL-2\"\n",
    "\n",
    "products = dataspace_dataframe_from_attributes(\n",
    "    collection=satellite,\n",
    "    start_date=start_date.strftime(\"%Y-%m-%d\"),\n",
    "    end_date=end_date.strftime(\"%Y-%m-%d\"),\n",
    "    aoi=poi,\n",
    "    attributes=[('cloudCover', 'le', 40)]\n",
    ")\n",
    "\n",
    "# convert to GeoPandas GeoDataFrame\n",
    "products_gdf = copernicus_df_to_gdf(products, strftime='iso')\n",
    "assert not products_gdf.empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sort and select the first row\n",
    "product = products_gdf.sort_values(\n",
    "    [\"cloudCover\", \"PublicationDate\"], ascending=[True, True]\n",
    ").head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ensuring that the tile is the one we are also providing via a direct\n",
    "# download link\n",
    "expected_name = \"S2A_MSIL1C_20240308T100841_N0510_R022_T32UQD_20240308T120958.SAFE\"\n",
    "assert product[\"Name\"].iloc[0] == expected_name, f\"Expected name: {expected_name}, but got name: {product['Name'].iloc[0]}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Would fail, because there are entries that cannot be JSON encoded!\n",
    "# product.explore()\n",
    "# Only visualize the relevant columns and convert them if necessary, for us 'title', 'summary', and 'geometry' are sufficent\n",
    "# if you exclude the geometry column, it cannot be drawn on the map!\n",
    "product[[\"Name\", \"cloudCover\", \"geometry\", 'OriginDate']].explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Why does our tile look so oddly shaped? What does it mean? How is our data formatted? Does the _missing/invalid_ region have specific values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting regions of tiles \n",
    "\n",
    "We can use the `osmnx` library to retrieve the area of interest by looking up a name-identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import osmnx\n",
    "import rasterio.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "berlin_char_gdf = osmnx.geocode_to_gdf(\"Berlin, Charlottenburg\")\n",
    "berlin_char_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "berlin_char_gdf.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this shape, we can create the bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "berlin_char_gdf_rect = berlin_char_gdf.envelope\n",
    "berlin_char_gdf_rect.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "?rasterio.mask.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_s2_jp2_data_with_clipping(\n",
    "    band_data_path: Path, clip_geoseries: geopandas.GeoSeries, envelope: bool = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Given the `band_data_path` to a JP2000 encoded GeoTIFF file, return those parts that are overlapping with the\n",
    "    `clip_geoseries` GeoSeries. By default, the bounding box (`envelope`) of the geometry will be used to define the\n",
    "    region of interest.\n",
    "    \"\"\"\n",
    "    # open the GeoTIFF file which also contains the CRS metadata\n",
    "    with rasterio.open(band_data_path) as data:\n",
    "        # ensure that the data is using the same coordinate reference system and reproject if they don't\n",
    "        reprojected_geoseries = clip_geoseries.to_crs(data.crs)\n",
    "        # use the bounding box if `envelope` is set => Make sure that the matrix we get back can contain\n",
    "        # only valid values\n",
    "        reprojected_geoseries = (\n",
    "            reprojected_geoseries.envelope if envelope else reprojected_geoseries\n",
    "        )\n",
    "        # Use crop to only return the matrix that contains our region of interest\n",
    "        # Use `all_touched=True` to make sure that the border is also conisdered \"inside\" the region of interest\n",
    "        out_img, _out_transform = rasterio.mask.mask(\n",
    "            data, reprojected_geoseries, crop=True, all_touched=True\n",
    "        )\n",
    "        # drop singleton axes\n",
    "        out_img = out_img.squeeze()\n",
    "    return out_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class S2TileReader:\n",
    "    def __init__(self, directory: Path):\n",
    "        \"\"\"\n",
    "        Initialize the reader with a directory containing the SAFE file of a Sentinel-2 product.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        directory : Path\n",
    "            The directory containing the SAFE file of a Sentinel-2 product.\n",
    "        \"\"\"\n",
    "        assert directory.is_dir(), f\"{directory} is not a directory\"\n",
    "        self.image_files = list(directory.glob(f\"**/IMG_DATA/*.jp2\"))\n",
    "        if len(self.image_files) == 0:\n",
    "            self.image_files = list(directory.glob(f\"**/IMG_DATA/R60m/*.jp2\"))\n",
    "            self.image_files.extend(list(directory.glob(f\"**/IMG_DATA/R20m/*.jp2\")))\n",
    "            self.image_files.extend(list(directory.glob(f\"**/IMG_DATA/R10m/*.jp2\")))\n",
    "        self.band2file_mapping = self._bands()\n",
    "        self.bands = sorted(self.band2file_mapping.keys())\n",
    "        print(f\"{len(self.band2file_mapping)} images found in {directory}\")\n",
    "\n",
    "    def _bands(self):\n",
    "        \"\"\"\n",
    "        Extract the band names from the image files and create a mapping from band name to file path.\n",
    "\n",
    "        Example:\n",
    "        {\n",
    "            \"B01\": Path(\"path/to/B01.jp2\"),\n",
    "            \"B02\": Path(\"path/to/B02.jp2\"),\n",
    "            ...\n",
    "        }\n",
    "        or if the product has multiple resolutions:\n",
    "        {\n",
    "            \"B01_60m\": Path(\"path/to/R60m/B01.jp2\"),\n",
    "            \"B02_10m\": Path(\"path/to/R10m/B02.jp2\"),\n",
    "            ...\n",
    "        }\n",
    "        \"\"\"\n",
    "        return {\"_\".join(x.stem.split(\"_\")[2:]):x for x in self.image_files}\n",
    "\n",
    "    def read_band(self, band: str):\n",
    "        \"\"\"\n",
    "        Read the data of a specific band. The data is returned as a numpy array. If the band is a single channel,\n",
    "        the array will have shape (height, width). If the band is a multi-channel band, the array will have shape\n",
    "        (height, width, channels).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        band : str\n",
    "            The name of the band to read. Must be one of the bands in the product.\n",
    "            Use the `bands` attribute to see the available bands.\n",
    "        \"\"\"\n",
    "        assert band in self.bands, f\"Band {band} invalid. Please select one of {self.bands}\"\n",
    "        img_path = self.band2file_mapping[band]\n",
    "        with rasterio.open(self.band2file_mapping[band]) as f:\n",
    "            data = f.read()\n",
    "        if data.shape[0] == 1:\n",
    "            return data.squeeze(0)\n",
    "        elif data.shape[0] == 3:\n",
    "            return np.transpose(data, (1, 2, 0))\n",
    "\n",
    "    ###\n",
    "\n",
    "    # Add new reading band with clipping function\n",
    "    def read_band_data_with_clipping(\n",
    "        self, band: str, clip_geoseries: geopandas.GeoSeries, envelope: bool = True\n",
    "    ) -> np.ndarray:\n",
    "        assert band in self.bands, f\"Band {band} invalid. Please select one of {self.bands}\"\n",
    "        img_path = self.band2file_mapping[band]\n",
    "        return read_s2_jp2_data_with_clipping(img_path, clip_geoseries, envelope=envelope)\n",
    "\n",
    "\n",
    "# re-initialize\n",
    "s2_reader = S2TileReader(unzipped_dir)\n",
    "band03_data = s2_reader.read_band(\"B03_10m\")\n",
    "band03_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clipped_band03_data = s2_reader.read_band_data_with_clipping(\n",
    "    \"B03_10m\", berlin_char_gdf.geometry, envelope=True\n",
    ")\n",
    "clipped_band03_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clipped_rgb_arr = np.stack(\n",
    "    [\n",
    "        s2_reader.read_band_data_with_clipping(b, berlin_char_gdf.geometry)\n",
    "        for b in (\"B04_10m\", \"B03_10m\", \"B02_10m\")\n",
    "    ],\n",
    "    axis=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vis(clipped_rgb_arr, quant_norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KGPyLgKnIPv"
   },
   "source": [
    "## Inspecting spectral signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LCLU: Land-Cover Land-Use\n",
    "lclu_gdf = geopandas.GeoDataFrame(\n",
    "    {\"type\": [\"water\", \"airport\", \"forest\"]},\n",
    "    geometry=[\n",
    "        # Took the points from latlong.net\n",
    "        Point(13.175955, 52.456009),\n",
    "        Point(13.508517, 52.380236),\n",
    "        Point(13.212926, 52.478834),\n",
    "    ],\n",
    "    crs=\"epsg:4326\",\n",
    ")\n",
    "lclu_gdf.reset_index().explore(marker_type=\"marker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# depends on product type and used image directory!\n",
    "AVAILABLE_BANDS = (\n",
    "    \"B01_20m\",\n",
    "    \"B02_10m\",\n",
    "    \"B03_10m\",\n",
    "    \"B04_10m\",\n",
    "    \"B05_60m\",\n",
    "    \"B06_20m\",\n",
    "    \"B07_20m\",\n",
    "    \"B08_10m\",\n",
    "    \"B09_60m\",\n",
    "    \"B11_20m\",\n",
    "    \"B12_20m\",\n",
    "    \"B8A_20m\",\n",
    ")\n",
    "\n",
    "\n",
    "def read_points_from_tile(\n",
    "    s2_reader: S2TileReader,\n",
    "    points_series: geopandas.GeoSeries,\n",
    "    bands: Sequence[str] = AVAILABLE_BANDS,\n",
    ") -> np.ndarray:\n",
    "    if set(lclu_gdf.geom_type) != {\"Point\"}:\n",
    "        raise ValueError(\"Only point geometries are allowed!\")\n",
    "\n",
    "    return np.array([s2_reader.read_band_data_with_clipping(b, points_series) for b in bands])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lclu_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example of pandas `query` command\n",
    "water_spectral_sig = read_points_from_tile(s2_reader, lclu_gdf.query(\"type == 'water'\"))\n",
    "airport_spectral_sig = read_points_from_tile(\n",
    "    s2_reader, lclu_gdf.query(\"type == 'airport'\").geometry\n",
    ")\n",
    "forest_spectral_sig = read_points_from_tile(s2_reader, lclu_gdf.query(\"type == 'forest'\").geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The following figure takes a very naïve approach and assumes that the selected pixels are noise free, high-resolution, etc.\n",
    "# but it is sufficient to get a clear understanding of general form of different spectral reflectance curves.\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(AVAILABLE_BANDS, forest_spectral_sig, \"o-\", label=\"forest\", linewidth=2, color=\"green\")\n",
    "plt.plot(AVAILABLE_BANDS, airport_spectral_sig, \"o-\", label=\"airport\", linewidth=2, color=\"gray\")\n",
    "plt.plot(AVAILABLE_BANDS, water_spectral_sig, \"o-\", label=\"water\", linewidth=2, color=\"blue\")\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brg-cl6PETK-"
   },
   "source": [
    "### Course exercise\n",
    "\n",
    "Select 3 different point (pixels) and plot their spectral signitures.\n",
    "Select those points either by using [LatLong.net](latlong.net) or using the `osmx` library and applying the correct function to the geometries to get a `Point`.\n",
    "\n",
    "Feel free to play around with the variuous different GeoDataFrame functions as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5KB-60d8EG9I"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab01.ipynb",
   "provenance": [
    {
     "file_id": "1iflYcaSB5m5qeFU5UMvFKX73OsLFVCbh",
     "timestamp": 1618818796742
    }
   ]
  },
  "interpreter": {
   "hash": "d950cc255805eca97bc9adaef38440cdd4d88c80fff2afb10853f0ffb81a073c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
